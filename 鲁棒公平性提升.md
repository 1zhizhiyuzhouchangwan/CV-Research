### Towards Fairness-Aware Adversarial Learning  面向公平性的对抗学习  

###### 2024.12.5

24.cvpr

面向公平性的对抗学习   

adversarial training (AT)对抗训练AT

#### ABSTRACT

> 虽然**对抗性训练（AT）**已被证明在增强模型的**鲁棒性**方面是有效的，但最近发现的鲁棒性问题尚未得到很好的解决，即**鲁棒准确性**在不同类别之间存在显着差异。

​	对抗性训练（Adversarial Training, AT）是一种通过在训练过程中引入对抗样本来增强模型鲁棒性的方法。对抗样本是那些经过精心设计的微小扰动，这些扰动对人类来说可能难以察觉，但却足以导致机器学习模型做出错误的预测。

但最近发现的鲁棒性问题尚未得到很好的解决，即**鲁棒准确性**在**不同类别之间存在显着差异**。

==这种类别间的差异意味着模型对某些类别的样本具有较高的鲁棒性，而对其他类别的样本则相对较弱。==例如，在某些情况下，“猫”这一类别可能比其他类别具有更高的鲁棒性。这种差异可能导致模型在面对特定类别的对抗样本时更容易受到攻击，尤其是当这些类别是关键类别时，比如在自动驾驶中识别“人类”类别

对抗性训练可能会增加这种类别间的差异。<u>（在对抗性训练的过程中，虽然模型的整体鲁棒性得到了提升，但这种提升可能并不是均匀分布在所有类别上的。具体来说，模型可能在某些类别上变得更加鲁棒，而在其他类别上则可能没有得到同样程度的改善，甚至可能变得更加脆弱。）</u> 这意味着，尽管模型的整体鲁棒性可能得到提高，但某些类别的样本可能会变得更加脆弱，从而增加了模型在实际应用中的潜在风险。这表明，仅仅关注模型的平均鲁棒性可能不足以确保模型在所有类别上都具有足够的鲁棒性，特别是在那些关键类别上。

因此，为了解决这一问题，研究人员正在探索新的技术和方法，以减少类别间的鲁棒性差异，并提高模型在所有类别上的鲁棒性。这可能包括改进对抗样本的生成方法、调整训练参数、以及结合正则化技术等。未来的研究将继续探索如何通过对抗性训练来提高模型的鲁棒性，同时确保不同类别之间的鲁棒性差异最小化，以构建更加稳定和可靠的人工智能系统。

###### 2024.12.6

在本文中，我们不再统一地评估模型的**平均类性能**，而是通过**考虑在各个类中的最坏情况**分布来深入研究鲁棒公平性问题。通过考虑不同类别的最坏情况分布。我们提出了一种新的学习范式，称为**公平意识对抗学习（FAAL）**==Fairness-Aware Adversarial Learn-ing (FAAL)==

> 传统对抗训练的流程
>
> 传统的对抗训练（Adversarial Training）是一种在机器学习领域，尤其是深度学习中常用的技术，旨在提高模型对对抗性攻击（Adversarial Attacks）的鲁棒性。对抗性攻击是指通过在输入数据中添加微小的、不易察觉的扰动，使得模型做出错误的预测。对抗训练的核心思想是让模型在训练过程中接触到这些对抗性样本，从而学会识别和抵抗这些攻击。
>
> 以下是对抗训练的基本步骤：
>
> 1. **数据准备**：准备原始的训练数据集。
> 2. **生成对抗样本**：对于每个训练样本，使用特定的方法（如Fast Gradient Sign Method, FGSM）生成对抗样本。这些样本在视觉上与原始样本几乎无法区分，但会导致模型做出错误的预测。
> 3. **训练模型**：将原始样本和对抗样本一起用于训练模型。这样，模型在训练过程中会接触到对抗样本，并学习如何正确分类这些样本。
> 4. **评估模型**：在对抗样本上评估模型的性能，以测试其对对抗攻击的鲁棒性。
> 5. **迭代优化**：根据评估结果，调整模型参数或训练策略，以进一步提高模型的鲁棒性。
>
> 对抗训练的优点是能够直接增强模型对对抗样本的识别能力，但它也有一些局限性，比如可能会增加模型的训练难度和过拟合的风险。此外，对抗训练通常只能提高模型对特定类型攻击的鲁棒性，而对未知或更复杂的攻击可能仍然脆弱。因此，研究人员还在探索其他方法，如对抗性防御策略和鲁棒性度量，以提高模型的整体安全性。

也就是本文中提出的方法旨在找出不同类别中最坏的（即最不利的）数据分布情况。**在这种最坏情况下，他们的方法能够保证以很高的概率获得性能的上界（即最佳可能的性能）。**这里的“最坏分布”指的是在所有可能的数据分布中，对模型性能影响最负面的那种分布。

> In particular, FAAL can fine-tune an
> unfair robust model to be fair within only two epochs, with-
> out compromising the overall clean and robust accuracies.

特别的，FAAL能够在两个epoch之内就对模型进行微调从而使不公平的鲁棒模型调整成公平的，还是在不危害整体的干净程度和鲁棒性的情况下//（听起来好厉害）

#### 1.Introduction

这里列举了几个例子还有对抗训练的鲁棒性的例子，这里引用一篇论文To be Robust or to be Fair: Towards Fairness in Adversarial Training，说明鲁棒性和公共性之间有很复杂的联系，

问题：自动驾驶系统中，识别那里是道路很成功，但识别道路上的行人这一点不太好，（在不同的类别上所表现的性能差异性大，但是这种差异性也同时影响了模型的性能），识别不出来人也可呢个会影响驾驶人和行人的安全，所以我们需要解决的就是这个问题，怎么提升不同类上的鲁棒公平性的问题。

> 因此，通过评估超出平均水平的最坏情况鲁棒性来确保一致、公平的模型性能以对抗对抗性攻击至关重要。这提供了比平均性能更准确的评估，认识到模型的局限性，同时确保在现实世界的应用程序中跨不同类别的可靠性。

> That is, the robust accuracy diverges significantly across
> classes. Furthermore, it is noteworthy that even though the
> class “cat” attains the lowest clean and robust accuracy, the
> most significant disparity between clean and robust accu-
> racy arises in the case of the class “deer”. This finding
> highlights an inconsistency between the clean and adver-
> sarial performances, where the robust accuracy on different
> classes illustrates more severe diverges in the model.

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20241206143811496.png" alt="image-20241206143811496" style="zoom:67%;" />

这里引入了一个实验来解释鲁棒公平性之间的差异性。

简单来说就是：

Wide-ResNet34-10模型在CIFAR-10数据集上的类-wise（即针对每个类别）准确度，包括干净数据（clean accuracy）和对抗攻击下的鲁棒性准确度（robust accuracy）

文中一共有四条线：

clean acc 没有对训练数据进行任何干扰下的标准情况，这是一般情况下模型性能的评价指标

这里的AA acc 是在auto attack自动攻击下的模型准确性

还有标准训练和对抗训练，标准训练下不考虑添加的扰动的攻击样本，对抗训练则相反。

然后我们来分析这四条线和对应的上下文：

蓝色虚线：标准训练不添加扰动样本，可以看出准确率非常的高，几乎在每一类上面的表现都非常之好。

蓝色实线，几乎每个类别都直接变成0了，说明普通的非对抗模型对于一些混乱的数据几乎是毫无抵抗力的

橘色虚线：对抗训练的在干净样本上的acc（对汽车额acc几乎是1，但是对于cat这个分类不足80%）

橘色实线：对抗训练在经历了auto attack之后的准确率。

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20241206151455136.png" alt="image-20241206151455136" style="zoom:50%;" />

虽然猫🐱类别attains the lowest clean and robust accuracy，但是在引入对抗扰动样本之后，鲁棒差异性最大是鹿🦌类别。

这里就表明clean 和鲁棒性之间的分歧，虽然鹿类clean acc比较好，但是他的鲁棒差异性是最差的。

这里就是鲁棒公平性问题，引入平均鲁棒性和最差鲁棒性之间的差异。

目前已经提出的解决鲁棒公平性的方案：这些努力要么解决对抗性示例生成的鲁棒公平性，要么根据经验调整类权重。

对抗训练和对抗攻击是两个阶段的工作

引入思考：为什么重新加权策略在缓解鲁棒公平问题方面是有效的。

> 直观地说，在没有重新加权的情况下进行对抗训练的模型无法实现**高的最差类鲁棒准确性**，因为它平等地对待所有类，但忽略了替换的对抗示例可能会给最终模型带来**偏差**的事实。

**高最弱类别鲁棒性准确度（High Worst-Class Robust Accuracy）**：指的是模型在面对对抗样本时，对于数据集中最难以正确分类的类别（即最弱类别），仍能保持较高的准确度。

**引入偏差（Introduce Bias）**：如果在训练过程中没有对某些类别的样本进行适当的加权，那么模型可能会偏向于某些容易分类的类别，而忽视了那些难以分类的类别，从而导致模型在整体上存在偏差。

其他人用启发式的重加权。这里想引入基于优化算法的重加权来解决（因为引入对抗扰动对结果造成的可能的过拟合问题）所以就提到了==Distributional Robust Optimization(DRO)==似乎是一个合理的方法。DRO没有假设固定的统一数据分布，而是承认现实世界数据中固有的分布不确定性，提供了一个更具弹性和适应性的模型结构。

引入DRO之后，不是手动或者凭借过去的经验对权重进行调整，而是利用一些约束条件让DRO自己去学习到满足这些条件的权重。这里的约束条件就是预定义约束，**预定义约束（Pre-defined Constraints）**：在DRO框架中，研究者会设定一些约束条件，以确保学习到的权重满足特定的要求，比如保持类别间的公平性或满足特定的性能标准。用这些权重去面对一些未知的风险。

###### 2024.12.09

#### 2.related work

对抗训练中对数据添加一些肉眼不可见但是对机器影响很大的扰动，这些扰动**导致训练的模型边界和真实的模型边界不一致**。这个差异就是对抗样本的空间。

存在鲁棒性和鲁棒公平性之间的**折衷**，较大的扰动半径会造成比较大的方差。

扰动半径，模型能够正确分类的最大扰动范围，较大的扰动半径意味着模型对输入数据的小变化更加的不敏感，从而拥有更好的鲁棒性

这个折衷。。。。找到了一篇nips的论文还没看。

因为真实决策边界和模型分类的曲线是不一致的，不重合的区域就会导致模型分类出错。

引入正则化项，减轻折中，整出了**FAT**

调整每个类的攻击强度和难度，**以生成决策边界附近的样本，从而更容易和更公平地进行模型学习，（？？为什么决策边界附近的样本是更容易被学习的？）**这个是**BAT**

> adjusts the attack strengths and difficulties of each class to
> generate samples near the decision boundary for easier and
> fairer model learning.

###### 2024.12.11

Wei等人。[40]提出了一个名为**CFA**的框架，该框架可以**自动为每个类定制特定的训练配置**，从而在保持平均性能的同时提高最差类的鲁棒性。

最近，Li和Liu [26]考虑了最差类鲁棒风险，他们提出了一个名为**WAT**（最差类对抗训练）的框架，并利用无悔（no-regret）动态来解决这个问题。

**DRO(分布鲁棒调优)**被视作是处理分布不确定性的一个工具。

#### 3.methology

**ERM**

###### 2024.12.30

> Empirical Risk Minimization
>
> 普通的预测，一个数据分布P，给出一个从输入到输出的映射y，在来一个损失函数Loss
>
> 经验风险最小化

进而引入一个最小最大的问题

这里便于理解，是让损失最大的那部分得到

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20241230190721151.png" alt="image-20241230190721151" style="zoom:67%;" />



https://radars.ac.cn/article/doi/10.12000/JR21048

25.2.20

https://plugin.sowise.cn/viewpdf/198_280b3c3a-c15a-40ed-a720-7b70740df260?_sowise_user=4486b3b9-cd24-4e73-8091-7872008b9156&_language=zh&_websitePDFPreviewURL=https://radars.ac.cn//cn/article/pdf/preview/10.12000/JR21048.pdf

一个有关对抗性鲁棒性的综述（21年）我居然完整的读完了这篇论文，我真的很厉害呢

这块关于对抗性鲁棒攻击的部分写的很清楚，学到了学到了：

现有深度卷积神经网络识别模型依赖统计学习，只有在训练数据和测试数据服从独立同分布的假设前提下泛化性能才能得到有效保证，深度卷积神经网络图像识别模型在面对多种不同类型的训练数据和测试数据间分布漂移时，预测性能水平会大大下降，缺乏对输入扰动的鲁棒性。

这里是分布不同造成的性能下降导致的鲁棒性。

由此，深度网络对于对抗是非常的脆弱的。

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250220164256218.png" alt="image-20250220164256218" style="zoom:67%;" />

在训练过程中都进行了**数据增广**，但变换后的数据与原始输入数据高度相关，且来自同样的数据分布，然而对抗样本通常呈现非相关和非同分布特性，显然的是，对原始数据进行数据增广变换后的数据对于对抗训练是无意义的。但是因为他也增加了数据的多样性，因此对于提升模型的鲁棒性是很有帮助的。

任务决策边界&&模型决策边界

任务决策边界的问题是对于每一类都分类正确的边界，但是模型的决策边界是训练出来的决策行为，比如我们用二位的线性模型去进行模型的训练，得到的模型决策边界是不能拟合非线性的任务决策边界的。

在这两种决策边界不重合的部分都是可以构造对抗样本的地方，尤其是在模型的参数特别的复杂，高维空间里面是很难让两者重合，这时候我们构造对抗样本就从这个不重合的区域构造。

**MSTAR图像目标识别性能评估策略**这个什么意思我也是不太明白

对抗样本生成过程中，向正常干净样本添加非随机噪声违背了模型训练过程中关于统计噪声的隐性假设。

解释：（AI解释的也太好了，相比之下我好像是真的什么都不懂。怎么办怎么办真的服了毕不了业怎么办）

对抗样本生成的过程、模型训练中的噪声假设是什么

对抗样本是通过在干净样本上添加特定的扰动（即非随机噪声）来欺骗模型的。这些扰动通常是通过优化算法生成的，比如FGSM或PGD，目的是让模型产生错误的预测。而传统的模型训练过程中，数据增强通常会添加随机噪声，比如高斯噪声、椒盐噪声等，这些噪声是符合某种统计分布的，目的是提高模型的泛化能力。

还没看完，继续看论文，ai有历史记录反正。

P7有关经验风险最小话的问题

前馈神经网络：只有一个方向没有反向传播的神经网络

前馈神经网络可以看做是一个 函数 ，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射。

与表征对抗样本分类情况相关的特征包括误分类率（Misclassification Ratio, MR）以及Average Confidence of Adversarial Class（ACAC）和Average Confidence of True Class（ACTC），前者是指将对抗样本错误分类的概率，后两个指标则分别描述将**对抗样本分类**到错误和正确的类别的平均置信度

还没研究出来的开放性问题截至20年：

目前关于对抗样本在理论上为何存在及其特性描述等基础性问题学术界研究还没有形成统一的认识。深度卷积神经网络图像识别模型对抗鲁棒性与模型泛化性、模型堕化噪声鲁棒性之间的关系在理论上和实践中仍需进一步研究。

https://blog.nsfocus.net/nist-ai-100-2e2023/

###### 对抗式机器学习攻击与缓解措施分类及术语



https://dds.sciengine.com/cfs/files/pdfs/view/2096-1146/8EF83C2EBC4A45FEA4A00CDB7B8107A7.pdf



http://fcst.ceaj.org/CN/PDF/10.3778/j.issn.1673-9418.2311117?token=2ea2c485a16a40a3b44ec5491a0d7a9e

一些对抗鲁棒性的前沿进展

最大最小问题

传统的对抗训练是为了找到输入数据的最差扰动，

FAAL的提出就是在传统的最大最小之间加一层最大，这一层最大通过DRO分布鲁棒优化来实现。

这篇论文主要是想使用DRO来解决鲁棒公平性的问题，但是有关DRO跟一个不确定的set有关，要解决DRO的问题首先要解决这种不确定性。

**中间最大化**：通过**分布鲁棒优化（DRO）**，寻找类别间的最坏分布权重，解决类别分布偏移问题

在传统对抗训练的中间阶段，即内部最大化和外部最小化之间，我们引入了一个**类分布对抗权重**，用于确定不同类别之间的学习方向，这可以通过利用分布鲁棒优化来最佳解决。通过将该权重合并到外部最小化过程中以更新模型的参数，可以保护类（组）分布偏移以减轻鲁棒公平性问题。

定理1指出，在给定足够多的样本n的情况下，L_FAAL作为估计的损失，是实际测试分布损失L的上界，并且这一结论具有高概率成立。这里的L代表在未知组分布偏移下的测试分布上的损失。也就是说，FAAL方法通过优化得到的权重分配，能够保证在未知分布变化的情况下，模型的表现不会低于预期，从而提供了对未知类别分布偏移的保护。

接下来，我需要理解为什么交叉熵损失不能很好地反映类别之间的差异，而CW margin损失更适合这一任务。交叉熵损失通常用于分类任务，它衡量的是模型预测概率分布与真实标签之间的差异。然而，在对抗训练中，尤其是关注不同类别鲁棒性差异的情况下，交叉熵可能无法充分捕捉到类别间的边界差异。例如，某些类别可能在对抗攻击下更容易被误分类，而交叉熵损失可能无法有效区分这些类别间的鲁棒性差异。

相比之下，CW margin损失（Carlini & Wagner损失）设计用于生成对抗样本，它直接关注模型在正确类别和最具竞争力错误类别之间的边际差异。具体来说，对于一个样本，CW损失定义为正确类别logit与最大错误类别logit之间的差距。这种损失更直接地反映了模型对对抗样本的鲁棒性，因为它关注的是模型在边界附近的表现，而不仅仅是整体的分类准确性。

在FAAL框架中，使用CW margin损失来计算每个类别的对抗损失，可以更精确地衡量不同类别在对抗攻击下的脆弱性。通过最大化这些类别损失，FAAL能够识别出那些在对抗攻击下表现较差的类别，并通过优化权重分配来增强这些类别的鲁棒性。同时，定理1的理论保证确保了这种优化过程在统计上是可靠的，即使面对未知的分布偏移，模型的表现也能保持在较高水平。

总结来说，定理1为FAAL方法提供了理论支持，确保其在处理未知分布偏移时的鲁棒性和公平性。而使用CW margin损失而非交叉熵损失，则是为了更精准地捕捉和优化不同类别在对抗攻击下的表现差异，从而有效提升最差类别的鲁棒性，实现公平的对抗训练。

**CW攻击**

使用不同的距离判断函数和扰动边际条件来判断攻击。

见笔记本25.3.3号的笔记，写的推导过程比较详细

4.1 用于增强鲁棒性的微调

这里引入了一个很大的表格做数据背书，整体就是说明FAAL比FRL强得多，FRL需要训练80个轮次才能达到的性能，FAAL只需要两个轮次就能达到。

WRN-34-10骨架

FRL预训练模型对模型进行微调，从而提升模型的鲁棒性

FRL基于TRADES [47]提出了两种增强鲁棒公平性的策略FRL-RWRM0.05 and FRL-RWRM0.07

FAAL能够快速微调最初缺乏公平性的健壮模型

有一个问题诶，table3，到底什么意思，没看出来性能好的点在哪里。。

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250307110129998.png" alt="image-20250307110129998" style="zoom:67%;" />

有点看出来了，Average Acc (Worst-class Acc) (%)  左边是干净样本，右边是加入对抗之后的，

在干净样本上，FAALtrades的最差类准确率高于FAALat和WATtrades，而且平均准确率只能说一般，但是对于添加了对抗的样本来说，FAALtrades的最差类准确率是最高的，平均准确率也挺高，就是在保证了平均准确率的基础上提升了最差类别的准确率，这正是实验最想看到的结果。

但是还有一个问题就是，为什么干净样本上面的准确率也是在大概55%呢，这个有点奇怪。

我们强调了FAAL与现有最先进作品的本质区别，包括FRL [43]，WAT [26]和CFA [40]。FRL和WAT都是基于TRADES的方法，需要单独的验证集来执行重新加权策略

FAAL最好用的一个点就是他只插入了一个完全独立的中间max，适用于任何的对抗网络训练，加入DRO，找我也是他相比其他两种方法的优势所在，因为其他的是针对TRADES的，只适用于trade基线之类的。

FAAL可以推广到任何对抗训练方法，因为我们的中间最大化是一个完全独立的组件，插入到流行的最小-最大框架中，所以它不限于任何对手，不像一些方法FRL和WAT仅限于TRADES变体。

25.3.7

### On the Tradeoff Between Robustness and Fairness

NeurIPS-2022-on-the-tradeoff-between-robustness-and-fairness-Paper-Conference

##### （FAT）

鲁棒性&公平性问题的折中

引入问题：

**AT（对抗训练）中不同扰动半径对鲁棒公平性的影响**尚未得到研究，这就提出了一个自然的问题：平均鲁棒性和鲁棒公平性之间是否存在权衡？结论：**随着扰动半径的增加，更强的AT将导致更大的鲁棒精度的类差异**。我们的理论还表明对抗训练比自然训练更容易导致更严重的鲁棒公平性问题。

the variance of class-wise robust accuracy 用这个来衡量鲁棒公平性

25.3.10

这里有两张图，图一和图二，一张关于类间差异，一张关于平均鲁棒准确性。在三个不同的网络模型上使用同一种攻击方法FGSM，不同的扰动半径得到不同的类间差异和平均鲁棒公平性，得出的结论是，扰动半径逐渐增大的过程中，类间差异也会越来越大，也就鲁棒公平性越来越差，但是整体的平均鲁棒性得到提升

在证实了这样的现象之后，为了找到平均鲁棒性和鲁棒公平性之间的平衡，提出了一种基于方差的正则化方法，理论上分析这种现象并且提出了改进的方法FAT，

> provide a potential explanation
> for it through linear model with mixture Gaussian distribution.
>
> 主要探讨的是扰动半径和鲁棒公平性之间的理论联系

这里明确的提出了自然样本上面的训练和对抗样本上面的训练的区别，

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250310152630477.png" alt="image-20250310152630477" style="zoom:67%;" />

自然训练的目标函数是简单的期望损失最小化，而对抗训练的目标函数是一个极小极大问题，**先最大化扰动带来的损失，再最小化这个最大损失。**

本文主要研究的就是这里的扰动，这里使用的是无穷范数，也叫做l∞-attack in AT

> We use var and rob.acc to denote the variance of class-wise robust accuracy, and average robust accuracy,respectively.
>
> 原来论文里面的var和acc是这两个意思。。。感觉我之前读的论文好像都反了。。
>
> var代表的是类别之间的鲁棒准确性差异，rob.acc才是平均的准确性哈哈哈哈。。。我说其他的论文我读起来都是怪怪的。。原来是脚注这种东西我都看错了。。

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250310165212347.png" alt="image-20250310165212347" style="zoom:67%;" />

如图所示，很明显的是cat,deer类的准确率相比horse,ship类来说就是很差，而且随着训练扰动半径的增大，cat，deer类别的准确率在下降，相反的是horse,ship类别的准确率却在逐渐上升。这就导致了类间鲁棒不公平的现象出现，而且随着扰动半径的逐渐增大，类间不公平的问题越来越严重。

下面进行理论分析：part5

高斯分布（正态分布）

笔记本上写了一丢丢，srds都没怎么看懂这些东西，越看推导的越懵逼

###### 25.3.11

5.3 类间的鲁棒公平性衡量:

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250311103259131.png" alt="image-20250311103259131" style="zoom:67%;" />

VCRA=variance ofclass-wise robust accuracy (VCRA)

类的鲁棒准确性与平均鲁棒准确性的方差再平均到每个类上面

对抗训练的鲁棒准确率方差（VCRA）与ε_train正相关

然后根据类间准确性差异和类的鲁棒性之间做出权衡，引入了一个最小化，最小化一个参数让函数求两者加权之后的最小化，也就是

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250311105245411.png" alt="image-20250311105245411" style="zoom:67%;" />

这就是论文的核心产出？？加一个参数平衡这两者再求一个最小化，拿一些实验和理论分析来给这个实验背书就行了？？。

有点奇怪，但是已经接受了。可能大部分重要额贡献在中间我看不懂的理论推导的部分。。真的是大大的不理解了。。

也算是又读完了一篇文章。

### FairDRO: Group fairness regularization via classwise robust optimization

https://www.sciencedirect.com/science/article/pii/S0893608024008207

##### fairDRO类鲁棒优化

**Group fairness**下次试试这个关键词搜索出来的有关鲁棒公平性的论文瞅瞅

现有的群体公平性感知训练方法分为两类：根据**某些规则重新加权代表性不足的群体**，或使用**正则化项**，如公平性度量或替代统计量的平滑近似

（怎么能这么强度的立马总结了我师兄论文的全部内容的。。感觉被骂了，但是能力没有办法骂回去。）

这里意思是一般两个类别之间不能同时到达最优点，也是我们需要解决的问题，

> which takes advantage of both categories through a classwise group
> distributionally robust optimization (DRO) framework
>
> 通过DRO框架利用每个类别的优势

为什么重加权之后会有封闭解的，封闭解的意思是唯一确定解吗？

我们得出三个理论结果：（i）正确重新加权的封闭形式解;（ii）使用代理损失的理由;以及（iii）我们方法的收敛性分析。

这篇看不太懂，先放一下，换一篇试试





### CFA: Class-wise Calibrated Fair Adversarial Training

到目前为止，大多数现有的工作集中在提高整体模型的鲁棒性，在训练和测试阶段平等对待每个类。尽管揭示了类之间鲁棒性的差异，但很少有作品试图在不牺牲整体鲁棒性的情况下使对抗训练在类级别上公平。

**问题**：某些类别（“困难类别”）在对抗攻击下表现脆弱，而现有方法可能以牺牲这些类别的鲁棒性为代价提升整体性能。

论文提出了一种新的框架CFA，通过为每个类别定制不同的训练配置，如扰动边界、正则化参数，并结合权重平均技术，来提升整体鲁棒性和公平性。这里也有点像师兄的文章，加参数和改变扰动的半径来调优

作者提出了CFA框架，包含三个部分：**类别定制扰动边界（CCM）、类别定制正则化（CCR）和公平感知权重平均（FAWA）。**CCM根据每个类别的训练准确率动态调整扰动边界，困难类别使用较小的边界以减少过拟合。CCR则调整TRADES中的正则化参数，平衡鲁棒性和准确率。FAWA在权重平均时排除那些最差类别鲁棒性低于阈值的检查点，以稳定训练过程。

（这个作者的思路好正常啊，是这样所以才是科研的是吗，要不然人家能每年都中论文呢，框架的每一个部分都是定制的，还是说实际都是加权值和调整参数只是名字起的比较唬人）

###### 25.3.12

2.2 考虑一个二分类问题，两个类别的难易程度有一定区别，这里引入的+1，-1和均匀采样有点像上一篇论文呢里面的高斯分布和P概率

先解释我理解的情况，x1,x2,x3...，x1有随机的概率p等于y,同时有另一半的概率1-p等于-y，后续的x2,x3...是采样自一个N分布，这个分布没太看懂，其中x1对于对抗扰动是鲁棒的，但是并不完美，x2,x3...对于微小的扰动是敏感的，In our model, we introduce some differences between the two classes by letting the probability of x1 = y correlate with its label y.我们通过让概率P与标签y的相关来引入这两类的不同之处。

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250312103003852.png" alt="image-20250312103003852" style="zoom:67%;" />

只看图片还是蛮清晰的，就是只看图片我有点看不太懂。

We set p+1 > p−1 in our model，这里的p+1和p-1代表的是正类和负类的概率，（也是服了自己，学了这么多的机器学习的东西但是连基本的符号都不知道什么意思。。我们蠢货是这样子的）==因此，对于类别y =+1，鲁棒特征x1更可靠，而对于类别y =-1，鲁棒特征x1噪声更大，它们的分类更多地依赖于非鲁棒特征x2，· · ·，xd+1==。*如果设定 p+1>p−1，可能表示正类是数据中的多数类（更常见），或模型需要更关注正类的鲁棒性*

这里的解释，y是lable，**x是特征不是样本**。。。不懂脑子里面都是在想写什么，怎么看了半天的feature还是会很习惯性的把当成样本的。在这个例子中，x1就可以代表鲁棒特征，剩下的x2,x3,x4...等可以代表非鲁棒特征

鲁棒特征与非鲁棒特征的定义

- **鲁棒特征（如x₁）**：指在对抗扰动下保持稳定的特征，通常与数据本质属性相关（如物体形状、关键结构）。这些特征不易被微小扰动破坏，因此分类更可靠。

- **非鲁棒特征（如x₂,…,x_{d+1}）**：对扰动敏感的特征，可能关联数据中的次要属性（如纹理、颜色、背景噪声）。这些特征易受攻击影响，但可能在某些场景下提供补充信息。

  假设任务为“猫 vs. 非猫”分类：

  - 

    正类（猫）

    ：

    - 鲁棒特征 x₁ = “耳朵形状”，清晰且稳定（对抗扰动难以改变耳朵的几何结构）。
    - 模型主要依赖 x₁ 分类，鲁棒性高。

  - 

    负类（非猫）

    ：

    - 鲁棒特征 x₁（如“耳朵形状”）可能缺失或模糊（如狗、汽车等多样对象）。
    - 对抗扰动可轻易使 x₁ 接近猫耳形状（如添加噪声使狗耳朵变形），迫使模型依赖非鲁棒特征（如背景中的草地纹理 x₂），但这类特征易被攻击者伪造。

好好好又来了SVM,支持向量机是吧，久远的记忆但是已经没有什么印象了。

SVM全称是**支持向量机**，是一种**监督学习算法**，主要用于分类和回归分析。但这里用户问的是分类器，所以重点在分类部分。

那SVM的核心思想是什么呢？我记得它是通过找到一个最优的超平面来分隔不同类别的数据，这个超平面能够最大化不同类别之间的间隔，也就是所谓的“最大边距”。这个超平面在二维空间中是一条直线，三维可能是一个平面，更高维则是超平面。核心思想是寻找一个**最优超平面**，最大化不同类别数据之间的分类间隔（即“最大间隔原则”），提升泛化能力。其关键特点如下：

- **超平面与间隔**：
  在特征空间中，SVM通过线性或非线性超平面（如直线、曲面）划分数据，目标是使**间隔（margin）​**最大化，即两类最近样本点（**支持向量**）到超平面的距离之和最大。
- **支持向量**：
  超平面的位置仅由距离最近的少数样本（支持向量）决定，这些样本定义了分类边界，使模型对噪声和异常值更具鲁棒性。

2.3理论见解

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250312174544854.png" alt="image-20250312174544854" style="zoom:67%;" />

A+1(f**w) 和 A−1(f**w)：**自然准确率**（Natural Accuracy）**

- 定义：分类器 f**w 在干净（未受对抗攻击）样本上的分类准确率。

- 

  解释

  ：

  - A+1(f**w)：正类（y=+1）样本被正确分类的比例。
  - A−1(f**w)：负类（y=−1）样本被正确分类的比例。

- 定理意义：
  A+1​(f**w​)>A−1​(f**w​) 表明，分类器对正类的自然分类性能优于负类，可能因正类的鲁棒特征（如 x1​）更稳定、更易被模型捕捉。

*R*+1(*f**w*) 和 *R*−1(*f**w*)：**鲁棒准确率**（Robust Accuracy）**

- **定义**：分类器 *f**w* 在对抗扰动样本上的分类准确率。

- 

  解释

  ：

  - *R*+1(*f**w*)：正类样本在对抗攻击下仍被正确分类的比例。
  - *R*−1(*f**w*)：负类样本在对抗攻击下仍被正确分类的比例。

- **定理意义**：
  *R*+1​(*f**w*​)>*R*−1​(*f**w*​) 表明，正类在对抗攻击下的鲁棒性更强，可能因其依赖的鲁棒特征（如 *x*1​）对扰动不敏感，而负类依赖的非鲁棒特征（如 *x*2​,…,*x**d*+1​）易被攻击破坏。

###### 25.3.14

这里的定理四，我看上去就是大概是函数的问题，增加扰动对于正例和反例的标准准确性和鲁棒准确性之间的改变是谁的下降速率更大的问题。

3.Observations on Class-wise Robustness 类之间的鲁棒性观察

以vanilla AT [16]和TRADES [30]为例，我们比较了训练配置中的两个关键因素：vanilla AT中的扰动裕度和TRADES中的正则化β。最后，揭示了最差类鲁棒性在训练过程中的波动效应，这种波动效应对对抗性训练的鲁棒公平性产生显著影响。

3.1不同的margins

训练了八个不同扰动半径的模型来对比实验

###### 25.3.17

在pGD-10上，几乎可以是margin越大，所造成的鲁棒性更高，我们**想用最小的margin得到较好的鲁棒性**（我以为哈）为什么要较小的margin呢，margin对于模型的影响是什么，这里的边界

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250317102734569.png" alt="image-20250317102734569" style="zoom:67%;" />

对于简单类来说，增大margin他的鲁棒性会增长，但是对于困难类，增大margin可能会让鲁棒性进一步降低，图示的下半部分的几条线就是这种情况，当margin>8/255的时候，随着margin的增大，鲁棒性不增反降。

因此这里就提出了对于不同的类别使用不同的margin，听起来特别科学。

3.2不同的正则化项

我们可以看到，在鲁棒性上偏置更多的权重（使用更大的β）会导致类之间的不同影响。具体地，对于简单类，提高β可以以几乎没有干净准确性降低为代价来提高它们的鲁棒性，而对于硬类（例如，类2，3，4），提高β只能获得有限的鲁棒性改善，但明显下降清洁精度

因此我这里的理解是对于不同的类别加入不同的鲁棒正则项，对于简单类别，提高正则项β可以提高鲁棒的准确性同时不牺牲干净样本的准确率，对于困难类却并非如此，两者是相当于一个反比，一个增加另一个就会被牺牲，所以我们要找到他们之间的临界点，给这种不同难度的类别上不同的正则项β，就能让简单类别的鲁棒性增加的同时，又让困难类的标准准确率不至于下降的太多。

不得不说，这个思路真的很人性，就是很容易让人理解他的动机而且思考过程和对比实验都设置的很合理。

3.3Fluctuation Effect 这里翻译为涨落效应

大致意思就是平均最差和方差会随着训练轮次有一定的变化，但是这三个类比的变化幅度不一致

4 公平对抗训练

公平意识权重平均（FAWA）。CCM和CCR为不同的类定制适当的训练配置，FAWA修改权重平均以提高和稳定公平性。

4.1 CCM

###### 25.3.19

> leverage the class-wise training accuracy as the measurement of difficulty.
>
> 充分利用类间训练准确率作为是否判定困难类的衡量标准
>
> PS:根据3.1的实验现象，我们现在需要做的就是判定哪些是简单类，哪些是困难类，然后根据这些来判定的结果，针对每个类选择合适的margin来每个类都有近似最优的效果。对于简单类我们可以让margin尽可能的大，因为边界很大也不会影响他们的鲁棒性，对于困难类，我们的问题是找到他的临界条件，找到阈值，才能有效的平衡鲁棒行和准确性。

对于第K类，在训练了N个轮次之后已经是达到了鲁棒准确性tk，我们想要在训练的下一个轮次对margin进行改变（基于tk），但是为了保证这个margin不是过于的小，我们引入了一个超参数，
$$
\epsilon_k \gets (\lambda_1 + t_k) \cdot \epsilon
$$

> Note that the calibrated margin k can adaptively converge to find the   proper range during the training phase,
>
> 校准后的边界可以自适应的改变，这是为什么，训练的结果吗

这里又引入了一段话，

比如说，如果现在的margin对于类别k来说是太小，那么就会输出比较高的acc,也就是tk，经过上面的公式运算之后就会增大这个类的margin，这里的“现在的边界”是校准后的边界，比如常用数据集C-10里面这个margin就是8/255.

4.2正则项CCR

Q:用鲁棒准确性衡量类是否困难？这个是为什么？有什么代表性，还是说约定俗成。

A:用**鲁棒准确性**衡量某个类别是否属于“困难类”（hard class）是一种在特定场景下具有**代表性和实用性的方法**，但并非绝对意义上的“约定俗成”。其背后的逻辑、适用场景及局限性如下：

 1. 为什么用鲁棒准确性衡量“困难类”？
鲁棒准确性（Robust Accuracy）指模型在对抗样本（如对抗攻击、噪声扰动、分布偏移等）下的分类准确率。当某个类别在鲁棒准确性显著低于标准准确性（Clean Accuracy）时，可能表明该类存在以下特性：
• **特征敏感性高**：模型依赖脆弱特征（如高频噪声、局部纹理）而非语义特征进行分类，易受微小扰动影响。
• **类别边界模糊**：在特征空间中与其他类别重叠度高，导致对抗攻击容易跨越决策边界。
• **数据质量差**：训练数据中该类样本少、标注噪声多或多样性不足，模型未学到本质特征。

例如，若“狗”类别的鲁棒准确性远低于标准准确性，可能说明模型对狗的识别依赖于背景（如草地）而非动物本体，因此易受对抗攻击干扰。

---

2. 鲁棒准确性作为困难类指标的代表性
在以下场景中，鲁棒准确性具有代表性意义：
• 对抗鲁棒性研究：研究模型在对抗攻击下的表现时，鲁棒准确性直接反映模型对特定类别的抗干扰能力。例如，Madry et al. (2018) 通过鲁棒性分析揭示模型对不同类别的敏感性差异。
• 类别诊断工具：通过对比标准准确性与鲁棒准确性，定位模型易受攻击的类别，指导数据增强或模型改进。例如，ImageNet中“蝴蝶”类别可能因依赖纹理而鲁棒性差，需增加形状特征的训练样本。
• 安全关键领域：在医疗、自动驾驶等场景中，鲁棒性差的类别可能隐含高风险，需优先优化。

---

**3. 局限性：并非“万能量尺”**
鲁棒准确性并非唯一或普适的困难类衡量标准，需结合其他指标综合判断：
• **领域依赖性**：对自然扰动（如光照变化）敏感的类别可能并非“困难类”，而是任务定义或数据采集的问题。
• **攻击类型影响**：不同对抗攻击方法（如FGSM、PGD）可能暴露不同类别的脆弱性，需多角度评估。
• **与人类认知差异**：某些类别对人类简单但对模型困难（如区分不同犬种），可能因模型特征提取能力不足，而非类别本身复杂。

---

**4. 替代或补充指标**
若需更全面评估类别困难程度，可结合以下方法：
1. **混淆矩阵**：观察特定类别的误分类模式（如常被误分为哪几类）。
2. **熵或置信度**：模型对某类预测的置信度分布（困难类预测置信度低且分布分散）。
3. **特征空间分析**：通过t-SNE或聚类分析，查看类别在特征空间中的分离度。
4. **数据复杂度指标**：如类别内样本多样性、类间相似度（通过余弦距离等）。

---

**总结**
• **是否代表性？** 是的，在对抗鲁棒性研究和安全关键场景中，鲁棒准确性是衡量困难类的有效指标。
• **是否约定俗成？** 并非绝对，需根据任务目标和领域特点选择指标。**建议结合鲁棒性分析与传统指标（如混淆矩阵）**，以全面诊断模型弱点。



以及这里的TRADES,我居然没看过任何一篇专门讲它的文章，他明明是使用的最广泛的。。我也挺离谱的，经典的文章一篇没看哈哈哈哈。。。还是我本来阅读了但是公式太多我忽略过去了。。已经不清楚了。



###### 找一篇。。

是我要读的下一篇，好累，能不能把他也复现出来看看

https://proceedings.mlr.press/v97/zhang19p.html

Theoretically Principled Trade-off between Robustness and Accuracy

稳健性和准确性之间的理论权衡，提出trades的地方，何尝不是另一种梦开始的地方。。



这里的加参数相比之前的目标函数只是多了一个分母，和上面的CCM的原因类似，

4.4 FAWA公平意识权重平均

意思是上下浮动太大的需要调整权重参数？三天前看的已经几乎全部忘记了，我的妈呀。。。

调整权重让模型训练变得稳定的方法有很多种，比如EMA，因此我们就是期待通过调整权重来让类别间鲁棒性变得更加稳定。

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250320102455852.png" alt="image-20250320102455852" style="zoom:67%;" />

谁不同意？这个效果真的很好很好。。

一下子全部平滑了，而且还有点理论基础在身上，就是很合理很合理的问题啊

我们从数据集中提取一个验证集，每个检查点在加权平均过程中被采用当且仅当其最差类鲁棒性高于δ。

ps:这里他的解决方案有点奇怪，意思是我们减少曲线波动的措施就是判定如果他的鲁棒性比某一个阈值差的话就把他这个checkpoint不计入图像？sorry这个有点像掩耳盗铃。。不知道我理解的对不对。这不是纯粹的为了画图好看不择手段？

检查点：在训练模型时，通常保存多个**检查点（checkpoints）**（即不同训练阶段的模型参数），用于后续集成或加权平均（如SWA，Stochastic Weighted Averaging）

**是否体现丢弃的检查点**：

**会保留所有检查点的数据点**。即使某个检查点后续被丢弃（未参与加权平均），其在训练过程中的性能仍会体现在折线图中。

**转折点数量不变**：丢弃的检查点仍会在图中显示为一个“转折点”（即该轮次的性能值），只是后续加权平均时被排除。

这里的==加权平均==是在干什么

###### 25.3.21

在深度学习中，加权平均通常有两种情况：一种是模型参数的加权平均，另一种是预测结果的加权平均。模型参数的加权平均，比如SWA，是在训练过程中对不同时间点的模型参数进行平均，以得到更鲁棒的模型。而预测结果的加权平均则是将多个模型的预测结果按权重结合，比如集成学习中的方法。

加权平均的具体步骤。首先，在训练过程中保存多个检查点；然后，使用验证集评估每个检查点的最差类别鲁棒性；筛选出符合要求的检查点；最后，对这些检查点的模型参数进行加权平均，得到最终的模型。权重可能根据每个检查点的性能或其他指标来确定，比如鲁棒性越高，权重越大。

 **加权平均的目的**

通过融合多个检查点的参数，提升模型的**鲁棒性**和**泛化能力**。具体来说：

- **对抗过拟合**：不同检查点可能在不同数据分布或攻击方式下表现不同，加权平均可平滑模型参数，减少对特定数据的敏感性。
- **提升鲁棒性下限**：筛选检查点时已排除最差类别鲁棒性低的模型，加权平均进一步综合优质检查点的优势。

cfa融合TRADES

> 对于其他的对抗训练方法我们也可以通过：we can still incorporate CFA by re-
> moving the CCR schedule specified for TRADES. More-
> over, we discuss the difference between our proposed CFA
> and other works
>
> 仍然可以通过删除为TRADES指定的CCR时间表来纳入CFA

文章对比了常用的对抗训练方法，比较现在CFA和他们之间的区别。

 Instance-wise Adversarial Training.&FRL,是不是很多这种对抗训练方法都是基于TRADES的，所以才会很多实验都是在TRADES和resnet-18上完成的对比试验之类的，所以如果我跑模型的话也需要跑这些对比试验和其他的实验方法是不是。这只是我的小小猜想。

预训练的残差18就是做对比实验的？

这样之后就是对数据的分析，实验得出这个方法对公平性提升很有帮助，对比了和其他方法的数据性差异。

<img src="C:\Users\李瑞瑞\AppData\Roaming\Typora\typora-user-images\image-20250321161150523.png" alt="image-20250321161150523" style="zoom:67%;" />

不仅性能能挺好的，而且还使用于普通的对抗训练，挺好。

开新文了啊哈哈哈哈哈啊哈

### Theoretically Principled Trade-off between Robustness and Accuracy

真正的TRADES来喽

在鲁棒性和准确性之间的权衡上，在这项工作中，我们将对抗性示例的预测误差（鲁棒误差）分解为自然（分类）误差和边界误差的总和，并使用分类校准损失理论提供了一个可微的上限，这被证明是所有概率分布和可测量预测变量上最严格的上限。

受我们理论分析的启发，我们还设计了一种新的防御方法TRADES，以对抗性鲁棒性和准确性。我们提出的算法在真实世界的数据集上进行了实验。该方法是我们参加NeurIPS 2018对抗性视觉挑战赛的基础，在该挑战赛中，我们在约2，000份提交中获得第一名，在平均`2扰动距离方面超过亚军方法11.41%。

###### 25.3.24



---



近年来，对抗性训练不仅在提升模型对抗攻击下的鲁棒性方面取得了显著进展，同时也引起了对模型公平性的关注。以下是一些最新的进展和研究方向：

1. **鲁棒性与公平性的权衡分析**
    研究者发现，在对抗性训练中，提升鲁棒性可能会对模型的公平性产生负面影响。例如，一些模型在面对不同群体（如性别、种族等）时，对抗性样本的干扰程度可能不同，导致模型在部分群体上表现不均衡。近期的工作正尝试通过理论分析和实验验证，揭示鲁棒性与公平性之间的内在冲突与互补关系。

2. **引入公平性约束的对抗性训练方法**
    为了解决上述权衡问题，部分研究提出在对抗性训练的目标函数中增加公平性约束或正则项。这样的方法旨在在训练过程中同时考虑模型对抗扰动的抵抗能力和对不同群体的平等对待，从而在鲁棒性和公平性之间取得平衡。

   师兄做的大概就是这个方向。

3. **双重对抗机制**
    一些最新方法采用双重对抗机制：一方面通过传统对抗性训练提升模型对抗攻击的鲁棒性；另一方面引入额外的对抗网络（如判别器）检测和减弱模型在不同群体之间的不公平现象。此类方法能够在生成对抗样本的同时，有针对性地调整模型对敏感属性的依赖，从而提升整体公平性。

4. **评估指标和基准数据集的建立**
    为了更准确地评估鲁棒公平性，研究者们正在开发专门的评估指标和基准数据集。这些工作旨在量化模型在对抗攻击下各群体间的性能差异，从而为后续改进提供依据。

5. **跨领域应用与实际部署**
    随着对抗性训练和公平性问题在诸如金融风控、医疗诊断等高风险领域的应用，越来越多的研究开始关注如何在实际应用场景中平衡鲁棒性与公平性。这些研究不仅考虑理论上的性能提升，也注重系统的可解释性和用户信任度。

---

